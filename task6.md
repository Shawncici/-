## 高斯噪声
高斯噪声是符合高斯正态分布的误差。一些情况下我们需要向标准数据中加入合适的高斯噪声会让数据变得有一定误差而具有实验价值。

## 降低过拟合的方法

​	（1）从数据入手，获得更多的训练数据。使用更多的训练数据是解决过拟合问题最有效的手段，因为更多的样本能够让模型学习到更多更有效的特征，减小噪声的影响。当然，直接增加实验数据一般是很困难的，但是可以通过一定的规则来扩充训练数据。比如，在图像分类的问题上，可以通过图像的平移、旋转、缩放等方式扩充数据；更进一步地，可以使用生成式对抗网络来合成大量的新训练数据。**思考：在音频数据上应该怎么办呢？？？**
​	（2）降低模型复杂度。在数据较少时，模型过于复杂是产生过拟合的主要因素，适当降低模型复杂度可以避免模型拟合过多的采样噪声。例如，在神经网络模型中减少网络层数、神经元个数等；在决策树模型中降低树的深度、进行剪枝等。
​	（3）正则化方法。给模型的参数加上一定的正则约束，比如将权值的大小加入到损失函数中。以L2正则化为例：在优化原来的目标函数C0的同时，也能避免权值过大带来的过拟合风险。
​	（4）集成学习方法。集成学习是把多个模型集成在一起，来降低单一模型的过拟合风险，如Bagging方法。

## 降低欠拟合的方法
​ （1）添加新特征。当特征不足或者现有特征与样本标签的相关性不强时，模型容易出现欠拟合。通过挖掘“上下文特征”“ID类特征”“组合特征”等新的特征，往往能够取得更好的效果。在深度学习潮流中，有很多模型可以帮助完成特征工程，如因子分解机、梯度提升决策树、Deep-crossing等都可以成为丰富特征的方法。
​	（2）增加模型复杂度。简单模型的学习能力较差，通过增加模型的复杂度可以使模型拥有更强的拟合能力。例如，在线性模型中添加高次项，在神经网络模型中增加网络层数或神经元个数等。
​	（3）减小正则化系数。正则化是用来防止过拟合的，但当模型出现欠拟合现象时，则需要有针对性地减小正则化系数。

## 集成学习
集成学习（Ensemble  learning）是使用一系列学习器进行学习，并使用某种规则把各个学习结果进行整合，从而获得比单个学习器显著优越的泛化性能。它不是一种单独的机器学习算法啊，而更像是一种优化策略。因为单个机器学习模型所能解决的问题有限，泛化能力差，但是通过构建组合多个学习器来完成学习任务往往能够获得奇效，这些学习器可以看成一个个基本单元，由他们组合最终形成一个强大的整体，该整体可以解决更复杂的问题，其思想可以形象的概括为**三个臭皮匠赛过诸葛亮**。
![image](https://user-images.githubusercontent.com/33819026/115526845-091ab600-a2c3-11eb-855b-5ec525f6392a.png)
集成学习是一种技术框架，其按照不同的思路来组合基础模型，从而达到其利断金的目的。目前，有三种常用的集成学习框架：bagging ，Boosting和stacking。

### 集成学习优势
　　1，个体学习器之间存在一定的差异性，这会导致分类边界不同，也就是说可能存在错误。那么将多个个体学习器合并后，就可以得到更加合理的边界，减少整体的错误率，实现更好的效果。
　　2，对于数据集过大或者过小的情况，可以分别进行划分和有放回的操作，产生不同的数据子集，然后使用数据子集训练不同的学习器，最终再合并成为一个强学习器；
　　3，如果数据的划分边界过于复杂，使用线性模型很难描述情况，那么可以训练多个模型，然后再进行模型的融合。
　　4，对于多个异构的特征集的时候，很难直接融合，那么可以考虑使用每个数据集构建一个分类模型，然后将多个模型融合。
  
### 常见的集成学习框架
　　根据上述所说的单个学习器的产生过程不同，集成学习大致可以分为两类：串行，并行
　　串行：个体学习器们的产生依赖彼此，比如当前学习器的产生依赖上一个学习器的参数，所以最终将单个学习器们组合形式可以看做是串行序列化方法，代表是Boosting。
　　并行：个体学习器间不存在强依赖关系，可以并行化同时产生，代表是Bagging。
　　下面就说一下三个常用的集成学习框架：
      1，用于减少方差的Bagging
      2，用于减少偏差的Boosting
      3，用于提升预测结果的Stacking
